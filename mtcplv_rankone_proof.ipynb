{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigorous proof on rank-one eigen value decomposition\n",
    "\n",
    "This notebook shows the proof that the rank-one value decomposition performed in mtcplv function is equal to averaging the squared single-channel PLVs\n",
    "\n",
    "\n",
    "### Hao Lu, Anahita Mehta, 04/16/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition in formula (5) in the paper, the complex PCA was performed through the \"Cholesky factorization\" procedure:  \n",
    "\n",
    "\\begin{equation*}\n",
    "M(f) = Q(f) \\Lambda(f) Q^H(f)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $Q(f)$ is the unitary matrix of complex eigenvectors of $M(f)$, $\\Lambda$ is the diagonal matrix of real eigenvalues (Bharadwaj & Shinn-Cunningham, 2014)\n",
    "\n",
    "However, Cholesky factorization is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = R^*R\n",
    "\\end{equation*}\n",
    "\n",
    "where A is a positive definite Hermitian matrix and R is the upper triangular with positive diagonal elements (Krishnamoorthy & Menon, 2013).\n",
    "\n",
    "Hence, the method applied should be the eigenvalue decomposition as the authors described the matrix was decomposed into matrix of eigenvectors and eigenvalues. In the python codes (https://github.com/SNAPsoftware/ANLffr/blob/master/anlffr/spectral.py), the matrix was decomposed with the eigen value decomposition function of the scipy package (scipy.linalg.eigen(), line 441 of spectral.py). Therefore, we believe that this should be a typo in the paper and the complex PCA was performed through eigenvalue decomposition and not Cholesky factorization. The rest of this proof will use eigen value decompostion. Here onwards, we provide a proof on results of the eigen value decomposition on a rank one matrix to explain how this is equivalent to averaging the squared single channel PLVs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Csd = np.outer(C[:, fi], C[:, fi].conj())\n",
    "vals = linalg.eigh(Csd, eigvals_only=True)\n",
    "plv[k, fi] = vals[-1] / nchans\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refering to the code above from the spectral.py, the $Csd$ is the outer product of a vector and its conjugate version. Let the $Csd$ be termed as $D$ and column C[:,fi] be termed as vector $c$ for convience. The outer product can be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "D = cc^H\n",
    "\\end{equation*}\n",
    "\n",
    "From this, we can confirm that the matrix Csd is rank one and any rank one matrix is singular. Therefore $\\lambda = 0$ is an eigen value and the eigen vector for $\\lambda=0$ can be any nonzero $v$ such that $Dv=0$\n",
    "\n",
    "For the nonzero eigenvalues, it should satisfy:\n",
    "\n",
    "$$\n",
    "Dv = \\lambda v   \\\\\n",
    "$$\n",
    "which  is  equivalent  to\n",
    "\n",
    "$$\n",
    "c(c^Hv) = \\lambda v\n",
    "$$\n",
    "\n",
    "Note that $c^Hv$ is a scalar, and $\\lambda$ is a scalar. \n",
    "Now, if we assume $\\lambda \\neq 0$, then this means that $v$ is a scalar multiple of $c$:\n",
    "\n",
    "$$\n",
    "v = c(c^Hv)/\\lambda\n",
    "$$\n",
    "\n",
    "Therefore, c itself is an eigenvector associated with $\\lambda$:\n",
    "\n",
    "$$\n",
    "Dc = \\lambda c\\\\\n",
    "cc^Hc = \\lambda c\\\\\n",
    "$$\n",
    "\n",
    "as $c$ is non-zero:\n",
    "\n",
    "$$\n",
    "cc^H = \\lambda\n",
    "$$\n",
    "\n",
    "Therefore, we only have one non-zero eigen value which equals to $cc^H$. As multiplication of a complex number and its conjugate equals to its squared magnitude:\n",
    "\n",
    "$$\n",
    "(a+bi)(a-bi) = a^2 + b^2\n",
    "$$\n",
    "\n",
    "$cc^H$ equals to the sum of squared magnitude of all elements in c. In particular, when c is a vector of the single channel complex version of PLV and the magnitude of the complex version of PLV equals to the traditional PLV by definition, the only none-zero eigen value is equal to the sum of the squared single-channel PLV.\n",
    "\n",
    "In the last line of the code cited above, the eigen value was divided by the number of channels, so the output is functionally equivalent to the average of squared single-channel PLV.\n",
    "\n",
    "The eigen vector extracted will be the normalized version of the original complex PLV vector $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Bharadwaj, H. M., & Shinn-Cunningham, B. G. (2014). Rapid acquisition of auditory subcortical steady state responses using multichannel recordings. Clinical Neurophysiology, 125(9), 1878-1888.\n",
    "\n",
    "Krishnamoorthy, A., & Menon, D. (2013, September). Matrix inversion using Cholesky decomposition. In 2013 signal processing: Algorithms, architectures, arrangements, and applications (SPA) (pp. 70-72). IEEE.\n",
    "\n",
    "https://www.physicsforums.com/threads/eigenvalues-of-a-rank-1-matrix.682216/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
